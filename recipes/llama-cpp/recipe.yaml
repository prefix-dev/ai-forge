context:
  version: "b2813"

recipe:
  name: llama-cpp
  version: ${{ version }}

source:
  url: https://github.com/ggerganov/llama.cpp/archive/refs/tags/${{ version }}.tar.gz
  sha256: 59bb58e9b5e424b8d45a8cce1f492035edbcca49bdc9c69e7478b0e2df5c0207
  patches:
    - if: osx
      then: discrete-device.patch

outputs:
  - package: 
      name: llama-cpp
    requirements:
      build:
        - ${{ compiler('cxx') }}
        - if: cuda
          then:
            - ${{ compiler('cuda') }}
        - ninja
        - cmake
      host:
        - openssl
        - if: cuda
          then:
            - libcublas-dev

    build:
      script: |
        if [[ "${{ cuda }}" == "true" ]]; then
          export CUDA_ARGS="-DLLAMA_CUDA=ON"
        fi

        cmake -GNinja -S . -B build ${CMAKE_ARGS} \
          -DLLAMA_BUILD_TESTS=OFF \
          -DLLAMA_METAL_EMBED_LIBRARY=ON \
          $CUDA_ARGS

        cmake --build build
        cmake --install build

  - package:
      name: llava-7b-q4
      version: "1.5"
    requirements:
      run:
        - llama-cpp

    source:
      url: https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-Q4_K.gguf?download=true
      sha256: c91ebf0a628ceb25e374df23ad966cc1bf1514b33fecf4f0073f9619dec5b3f9

    build:
      noarch: generic
      script:
        - mkdir -p $PREFIX/share/llama-cpp/models
        - cp llava-v1.5-7b-Q4_K.gguf $PREFIX/share/llama-cpp/models/llava-v1.5-7b-Q4_K.gguf